{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TBN.ipynb","provenance":[{"file_id":"https://github.com/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb","timestamp":1631852135633}],"collapsed_sections":["hYaJ6dvF0v7g","R122bwRNbTus"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bhOAxQyU0rhs"},"source":["Copyright 2017 Google LLC.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"hYaJ6dvF0v7g"},"source":["# MusicVAE: A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music.\n","### ___Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck___\n","\n","[MusicVAE](https://g.co/magenta/music-vae) learns a latent space of musical scores, providing different modes\n","of interactive musical creation, including:\n","\n","* Random sampling from the prior distribution.\n","* Interpolation between existing sequences.\n","* Manipulation of existing sequences via attribute vectors.\n","\n","Examples of these interactions can be generated below, and selections can be heard in our\n","[YouTube playlist](https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr).\n","\n","For short sequences (e.g., 2-bar \"loops\"), we use a bidirectional LSTM encoder\n","and LSTM decoder. For longer sequences, we use a novel hierarchical LSTM\n","decoder, which helps the model learn longer-term structures.\n","\n","We also model the interdependencies between instruments by training multiple\n","decoders on the lowest-level embeddings of the hierarchical decoder.\n","\n","For additional details, check out our [blog post](https://g.co/magenta/music-vae) and [paper](https://goo.gl/magenta/musicvae-paper).\n","___\n","\n","This colab notebook is self-contained and should run natively on google cloud. The [code](https://github.com/tensorflow/magenta/tree/master/magenta/models/music_vae) and [checkpoints](http://download.magenta.tensorflow.org/models/music_vae/checkpoints.tar.gz) can be downloaded separately and run locally, which is required if you want to train your own model."]},{"cell_type":"markdown","metadata":{"id":"R122bwRNbTus"},"source":["# Basic Instructions\n","\n","1. Double click on the hidden cells to make them visible, or select \"View > Expand Sections\" in the menu at the top.\n","2. Hover over the \"`[ ]`\" in the top-left corner of each cell and click on the \"Play\" button to run it, in order.\n","3. Listen to the generated samples.\n","4. Make it your own: copy the notebook, modify the code, train your own models, upload your own MIDI, etc.!"]},{"cell_type":"markdown","metadata":{"id":"ZLfb2a_12wcj"},"source":["# Environment Setup\n","Includes package installation for sequence synthesis. Will take a few minutes.\n"]},{"cell_type":"code","metadata":{"id":"PfRDVhNs3UFx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632476820695,"user_tz":360,"elapsed":13614,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"a81ca961-91ef-48f3-cc3f-e61d5f546121"},"source":["#@title Setup Environment\n","#@test {\"output\": \"ignore\"}\n","\n","import glob\n","\n","BASE_DIR = \"gs://download.magenta.tensorflow.org/models/music_vae/colab2\"\n","\n","print('Installing dependencies...')\n","!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n","!pip install -q pyfluidsynth\n","!pip install -qU magenta\n","\n","# Hack to allow python to pick up the newly-installed fluidsynth lib.\n","# This is only needed for the hosted Colab environment.\n","import ctypes.util\n","orig_ctypes_util_find_library = ctypes.util.find_library\n","def proxy_find_library(lib):\n","  if lib == 'fluidsynth':\n","    return 'libfluidsynth.so.1'\n","  else:\n","    return orig_ctypes_util_find_library(lib)\n","ctypes.util.find_library = proxy_find_library\n","\n","\n","print('Importing libraries and defining some helper functions...')\n","from google.colab import files\n","import magenta.music as mm\n","from magenta.models.music_vae import configs\n","from magenta.models.music_vae.trained_model import TrainedModel\n","import numpy as np\n","import os\n","import tensorflow.compat.v1 as tf\n","\n","tf.disable_v2_behavior()\n","\n","# Necessary until pyfluidsynth is updated (>1.2.5).\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","def play(note_sequence):\n","  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n","\n","def interpolate(model, start_seq, end_seq, num_steps, max_length=32,\n","                assert_same_length=True, temperature=0.5,\n","                individual_duration=4.0):\n","  \"\"\"Interpolates between a start and end sequence.\"\"\"\n","  note_sequences = model.interpolate(\n","      start_seq, end_seq,num_steps=num_steps, length=max_length,\n","      temperature=temperature,\n","      assert_same_length=assert_same_length)\n","\n","  print('Start Seq Reconstruction')\n","  play(note_sequences[0])\n","  print('End Seq Reconstruction')\n","  play(note_sequences[-1])\n","  print('Mean Sequence')\n","  play(note_sequences[num_steps // 2])\n","  print('Start -> End Interpolation')\n","  interp_seq = mm.sequences_lib.concatenate_sequences(\n","      note_sequences, [individual_duration] * len(note_sequences))\n","  play(interp_seq)\n","  mm.plot_sequence(interp_seq)\n","  return interp_seq if num_steps > 3 else note_sequences[num_steps // 2]\n","\n","def download(note_sequence, filename):\n","  mm.sequence_proto_to_midi_file(note_sequence, filename)\n","  files.download(filename)\n","\n","print('Done')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing dependencies...\n","Importing libraries and defining some helper functions...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n","Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n","  from numba.decorators import jit as optional_jit\n","/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n","Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n","  from numba.decorators import jit as optional_jit\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","Done\n"]}]},{"cell_type":"code","metadata":{"id":"5_FZshAiN3Es"},"source":["import os\n","import re\n","import tarfile\n","import tempfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzYp74Bn9YFw","executionInfo":{"status":"ok","timestamp":1632476821245,"user_tz":360,"elapsed":21,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"2ff7676c-7b1b-475f-f6d8-df92c437cccd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBHlK58Z9bT2","executionInfo":{"status":"ok","timestamp":1632476821246,"user_tz":360,"elapsed":17,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"da206299-15af-448b-d44e-af356fbc1b90"},"source":["%cd /content/drive/MyDrive/Magenta/magenta"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1KPdTL1Qxr1axhsrKGYsHmrgaYhBBxUzi/Magenta/magenta\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZvfVYl7DXjt","executionInfo":{"status":"ok","timestamp":1632474344268,"user_tz":360,"elapsed":177,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"87456e71-66c7-42e5-c5b1-553bc6870ea3"},"source":["ls data/checkpoints/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cat-mel_2bar_big.tar\n"]}]},{"cell_type":"markdown","metadata":{"id":"moLOftFqBS-0"},"source":["# 2-Bar Melody Model\n","\n","The pre-trained model consists of a single-layer bidirectional LSTM encoder with 2048 nodes in each direction, a 3-layer LSTM decoder with 2048 nodes in each layer, and Z with 512 dimensions. The model was given 0 free bits, and had its beta valued annealed at an exponential rate of 0.99999 from 0 to 0.43 over 200k steps. It was trained with scheduled sampling with an inverse sigmoid schedule and a rate of 1000. The final accuracy is 0.95 and KL divergence is 58 bits."]},{"cell_type":"code","metadata":{"id":"2XCPjwd6BVtm"},"source":["#@title Load the pre-trained model.\n","mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n","mel_2bar = TrainedModel(mel_2bar_config, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/mel_2bar_big.ckpt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnwRuUVs91SI"},"source":["# Create and load model"]},{"cell_type":"code","metadata":{"id":"SgyIKG9f9_IM"},"source":["# Parameters\n","config_name = 'cat-mel_2bar_big'\n","checkpoint_dir_or_path= '/content/drive/MyDrive/Magenta/magenta/data/checkpoints/cat-mel_2bar_big.tar'\n","batch_size = 512\n","var_name_substitutions = None\n","\n","# train params \n","train_dir = '/content/drive/MyDrive/Magenta/magenta/data/tmp/train-9-24-01/train/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYRo3YIjDlUg","executionInfo":{"status":"ok","timestamp":1632476821475,"user_tz":360,"elapsed":243,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"9f9763b1-f494-4621-aad7-ba900e1459f7"},"source":["config = configs.CONFIG_MAP[config_name]\n","if tf.gfile.IsDirectory(checkpoint_dir_or_path):\n","  checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir_or_path)\n","else:\n","  checkpoint_path = checkpoint_dir_or_path\n","config.hparams.batch_size = batch_size\n","\n","with tf.Graph().as_default():\n","  model = config.model\n","  model.build(\n","      config.hparams,\n","      config.data_converter.output_depth,\n","      is_training=True\n","  )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n","{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n","INFO:tensorflow:\n","Encoder Cells (bidirectional):\n","  units: [2048]\n","\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","INFO:tensorflow:\n","Decoder Cells:\n","  units: [2048, 2048, 2048]\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/music_vae/lstm_utils.py:145: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"]}]},{"cell_type":"code","metadata":{"id":"5AucJbAOIAX0"},"source":["# Input placeholders\n","temperature = tf.placeholder(tf.float32, shape=())\n","\n","if config.hparams.z_size:\n","  z_input = tf.placeholder(\n","      tf.float32, shape=[batch_size, config.hparams.z_size]\n","  )\n","else:\n","  z_input = None\n","\n","if config.data_converter.control_depth > 0:\n","  c_input = tf.placeholder(\n","      tf.float32, shape=[None, config.data_converter.control_depth]\n","  )\n","else:\n","  c_input = None\n","\n","inputs = tf.placeholder(\n","    tf.float32,\n","    shape=[batch_size, None, config.data_converter.input_depth]\n",")\n","controls = tf.placeholder(\n","    tf.float32,\n","    shape=[batch_size, None, config.data_converter.control_depth]\n",")\n","inputs_length = tf.placeholder(\n","    tf.int32,\n","    shape=[batch_size] + list(config.data_converter.length_shape)\n",")\n","max_length = tf.placeholder(tf.int32, shape=())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T0DFCaIKi62","executionInfo":{"status":"ok","timestamp":1632476833413,"user_tz":360,"elapsed":728,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"ffaed856-afaf-46ee-af95-d14117e11c41"},"source":["# Output placeholders\n","outputs, decoder_results = model.sample(\n","    batch_size,\n","    max_length,\n","    z=z_input,\n","    c_input=c_input,\n","    temperature=temperature\n",")\n","\n","if config.hparams.z_size:\n","  q_z = model.encode(inputs, inputs_length, controls)\n","  mu = q_z.loc\n","  mu = q_z.scale.diag\n","  z = q_z.sample()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  warnings.warn('`tf.layers.dense` is deprecated and '\n","/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n","/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  warnings.warn('`layer.add_variable` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/contrib/rnn.py:474: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Do not call `graph_parents`.\n"]}]},{"cell_type":"code","metadata":{"id":"Y77yjv7WNAH-"},"source":["var_map = None\n","if var_name_substitutions is not None:\n","  var_map = {}\n","  for v in tf.global_variables():\n","    var_namr = v.name[:-2]  # Strip ':0' suffix\n","    for pattern, substitution in var_name_substitutions:\n","      var_name = re.sub(pattern, substitution, var_name)\n","    if var_name != v.name[:-2]:\n","      tf.logging.info('Renaming `%s` to `%s`.', v.name[:-2], var_name)\n","    var_map[var_name] = v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxB6AoQ2Ocao","executionInfo":{"status":"ok","timestamp":1632478300828,"user_tz":360,"elapsed":27276,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"4f2a44d7-9d17-4aa2-e8e8-bd91e04ee275"},"source":[" # Restore graph\n","sess = tf.Session()\n","saver = tf.train.Saver(var_map)\n","if (os.path.exists(checkpoint_path) and\n","    tarfile.is_tarfile(checkpoint_path)):\n","  tf.logging.info('Unbundling checkpoint.')\n","  with tempfile.TemporaryDirectory() as temp_dir:\n","    tar = tarfile.open(checkpoint_path)\n","    tar.extractall(temp_dir)\n","    # Assume only a single checkpoint is in the directory.\n","    for name in tar.getnames():\n","      if name.endswith('.index'):\n","        checkpoint_path = os.path.join(temp_dir, name[0:-6])\n","        break\n","    saver.restore(sess, checkpoint_path)\n","else:\n","  saver.restore(sess, checkpoint_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Unbundling checkpoint.\n","INFO:tensorflow:Restoring parameters from /tmp/tmpivrs340r/cat-mel_2bar_big.ckpt\n"]}]},{"cell_type":"markdown","metadata":{"id":"VxUFsYHJSqZP"},"source":["# Start Training"]},{"cell_type":"code","metadata":{"id":"TN3K-ZSzSjl1"},"source":["tf.gfile.MakeDirs(train_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xv6KZB-BVJAo","executionInfo":{"status":"ok","timestamp":1632479074284,"user_tz":360,"elapsed":172,"user":{"displayName":"Anahita Doosti Sanjani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuC5ydWhsFH_bKze2iVn93Z5BW8mlDDwhN12E6vg=s64","userId":"11525648660123849199"}},"outputId":"a39eca7b-2f9a-46f2-ea5f-e0e635e1402f"},"source":["tf.config.list_logical_devices()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n"," LogicalDevice(name='/device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"I7Qmjb0jTY7K"},"source":["def train(train_dir,\n","          config,\n","          dataset_fn,\n","          checkpoints_to_keep=5,\n","          keep_checkpoint_every_n_hours=1,\n","          num_steps=None,\n","          master='',\n","          num_sync_workers=0,\n","          num_ps_tasks=0,\n","          task=0):\n","  print(\"These are the arguments====================================\")\n","  print(train_dir,\n","          config,\n","          dataset_fn,\n","          checkpoints_to_keep,\n","          keep_checkpoint_every_n_hours,\n","          num_steps,\n","          master,\n","          num_sync_workers,\n","          num_ps_tasks,\n","          task)\n","          #100 1 200000  0 0 0\n","  print(\"These are the arguments====================================\")\n","  \"\"\"Train loop.\"\"\"\n","  tf.gfile.MakeDirs(train_dir)\n","  is_chief = (task == 0)\n","  if is_chief:\n","    _trial_summary(\n","        config.hparams, config.train_examples_path or config.tfds_name,\n","        train_dir)\n","  with tf.Graph().as_default():\n","    with tf.device(tf.train.replica_device_setter(\n","        num_ps_tasks, merge_devices=True)):\n","\n","      model = config.model\n","      model.build(config.hparams,\n","                  config.data_converter.output_depth,\n","                  is_training=True)\n","\n","      optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n","\n","      hooks = []\n","      if num_sync_workers:\n","        optimizer = tf.train.SyncReplicasOptimizer(\n","            optimizer,\n","            num_sync_workers)\n","        hooks.append(optimizer.make_session_run_hook(is_chief))\n","\n","      grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n","      global_norm = tf.global_norm(grads)\n","      tf.summary.scalar('global_norm', global_norm)\n","\n","      if config.hparams.clip_mode == 'value':\n","        g = config.hparams.grad_clip\n","        clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n","      elif config.hparams.clip_mode == 'global_norm':\n","        clipped_grads = tf.cond(\n","            global_norm < config.hparams.grad_norm_clip_to_zero,\n","            lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n","                grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n","            lambda: [tf.zeros(tf.shape(g)) for g in grads])\n","      else:\n","        raise ValueError(\n","            'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n","      train_op = optimizer.apply_gradients(\n","          list(zip(clipped_grads, var_list)),\n","          global_step=model.global_step,\n","          name='train_step')\n","\n","      logging_dict = {'global_step': model.global_step,\n","                      'loss': model.loss}\n","\n","      hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n","      if num_steps:\n","        hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n","\n","      scaffold = tf.train.Scaffold(\n","          saver=tf.train.Saver(\n","              max_to_keep=checkpoints_to_keep,\n","              keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n","      tf_slim.training.train(\n","          train_op=train_op,\n","          logdir=train_dir,\n","          scaffold=scaffold,\n","          hooks=hooks,\n","          save_checkpoint_secs=60,\n","          master=master,\n","          is_chief=is_chief)\n","\n","\n","def evaluate(train_dir,\n","             eval_dir,\n","             config,\n","             dataset_fn,\n","             num_batches,\n","             master=''):\n","  \"\"\"Evaluate the model repeatedly.\"\"\"\n","  tf.gfile.MakeDirs(eval_dir)\n","\n","  _trial_summary(\n","      config.hparams, config.eval_examples_path or config.tfds_name, eval_dir)\n","  with tf.Graph().as_default():\n","    model = config.model\n","    model.build(config.hparams,\n","                config.data_converter.output_depth,\n","                is_training=False)\n","\n","    eval_op = model.eval(\n","        **_get_input_tensors(dataset_fn().take(num_batches), config))\n","\n","    hooks = [\n","        tf_slim.evaluation.StopAfterNEvalsHook(num_batches),\n","        tf_slim.evaluation.SummaryAtEndHook(eval_dir)\n","    ]\n","    tf_slim.evaluation.evaluate_repeatedly(\n","        train_dir,\n","        eval_ops=eval_op,\n","        hooks=hooks,\n","        eval_interval_secs=60,\n","        master=master)"],"execution_count":null,"outputs":[]}]}